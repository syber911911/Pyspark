{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5fbcea5-af4c-42f5-bf5e-106c20231a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re\n",
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder.master(\"local\").appName(\"twitterNews\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a244116-e029-4eab-93ec-8cba3f4ae34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2015', 'breast'), 2),\n",
       " (('2015', 'cancer'), 37),\n",
       " (('2015', 'risk'), 13),\n",
       " (('2015', 'test'), 14),\n",
       " (('2015', 'devised'), 1),\n",
       " (('2015', 'workload'), 1),\n",
       " (('2015', 'harming'), 1),\n",
       " (('2015', 'care'), 34),\n",
       " (('2015', 'poll'), 1),\n",
       " (('2015', 'short'), 2),\n",
       " (('2015', 'people'), 8),\n",
       " (('2015', 'heart'), 9),\n",
       " (('2015', 'greater'), 2),\n",
       " (('2015', 'approach'), 2),\n",
       " (('2015', 'against'), 3),\n",
       " (('2015', 'promising'), 2),\n",
       " (('2015', 'coalition'), 1),\n",
       " (('2015', 'undermined'), 1),\n",
       " (('2015', 'doctors'), 9),\n",
       " (('2015', 'review'), 6)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news1 = spark_session.sparkContext.textFile(\"./data/health_twitter/bbchealth.txt\") \\\n",
    "    .flatMap(lambda line: line.split('\\t')) \\\n",
    "    .map(lambda line: line.split('|')) \\\n",
    "    .map(lambda line: (line[1][26:], line[2])) \\\n",
    "    .mapValues(lambda line: re.sub('http://.*', '', line)) \\\n",
    "    .mapValues(lambda line: re.split('\\W+', line)) \\\n",
    "    .flatMap(lambda line: [(line[0], word) for word in line[1]]) \\\n",
    "    .filter(lambda line : len(line[1]) > 3) \\\n",
    "    .map(lambda line: ((line[0], line[1].lower()), 1)) \\\n",
    "    .reduceByKey(lambda word1, word2: word1 + word2)\n",
    "\n",
    "news2 = spark_session.sparkContext.textFile(\"./data/health_twitter/cbchealth.txt\") \\\n",
    "    .flatMap(lambda line: line.split('\\t')) \\\n",
    "    .map(lambda line: line.split('|')) \\\n",
    "    .map(lambda line: (line[1][26:], line[2])) \\\n",
    "    .mapValues(lambda line: re.sub('http://.*', '', line)) \\\n",
    "    .mapValues(lambda line: re.split('\\W+', line)) \\\n",
    "    .flatMap(lambda line: [(line[0], word) for word in line[1]]) \\\n",
    "    .filter(lambda line : len(line[1]) > 3) \\\n",
    "    .map(lambda line: ((line[0], line[1].lower()), 1)) \\\n",
    "    .reduceByKey(lambda word1, word2: word1 + word2)\n",
    "    \n",
    "news3 = spark_session.sparkContext.textFile(\"./data/health_twitter/cnnhealth.txt\") \\\n",
    "    .flatMap(lambda line: line.split('\\t')) \\\n",
    "    .map(lambda line: line.split('|')) \\\n",
    "    .map(lambda line: (line[1][26:], line[2])) \\\n",
    "    .mapValues(lambda line: re.sub('http://.*', '', line)) \\\n",
    "    .mapValues(lambda line: re.split('\\W+', line)) \\\n",
    "    .flatMap(lambda line: [(line[0], word) for word in line[1]]) \\\n",
    "    .filter(lambda line : len(line[1]) > 3) \\\n",
    "    .map(lambda line: ((line[0], line[1].lower()), 1)) \\\n",
    "    .reduceByKey(lambda word1, word2: word1 + word2)\n",
    "    \n",
    "news4 = spark_session.sparkContext.textFile(\"./data/health_twitter/everydayhealth.txt\") \\\n",
    "    .flatMap(lambda line: line.split('\\t')) \\\n",
    "    .map(lambda line: line.split('|')) \\\n",
    "    .map(lambda line: (line[1][26:], line[2])) \\\n",
    "    .mapValues(lambda line: re.sub('http://.*', '', line)) \\\n",
    "    .mapValues(lambda line: re.split('\\W+', line)) \\\n",
    "    .flatMap(lambda line: [(line[0], word) for word in line[1]]) \\\n",
    "    .filter(lambda line : len(line[1]) > 3) \\\n",
    "    .map(lambda line: ((line[0], line[1].lower()), 1)) \\\n",
    "    .reduceByKey(lambda word1, word2: word1 + word2)\n",
    "\n",
    "news5 = spark_session.sparkContext.textFile(\"./data/health_twitter/gdnhealthcare.txt\") \\\n",
    "    .flatMap(lambda line: line.split('\\t')) \\\n",
    "    .map(lambda line: line.split('|')) \\\n",
    "    .map(lambda line: (line[1][26:], line[2])) \\\n",
    "    .mapValues(lambda line: re.sub('http://.*', '', line)) \\\n",
    "    .mapValues(lambda line: re.split('\\W+', line)) \\\n",
    "    .flatMap(lambda line: [(line[0], word) for word in line[1]]) \\\n",
    "    .filter(lambda line : len(line[1]) > 3) \\\n",
    "    .map(lambda line: ((line[0], line[1].lower()), 1)) \\\n",
    "    .reduceByKey(lambda word1, word2: word1 + word2)\n",
    "\n",
    "news6 = spark_session.sparkContext.textFile(\"./data/health_twitter/gdnhealthcare.txt\") \\\n",
    "    .flatMap(lambda line: line.split('\\t')) \\\n",
    "    .map(lambda line: line.split('|')) \\\n",
    "    .map(lambda line: (line[1][26:], line[2])) \\\n",
    "    .mapValues(lambda line: re.sub('http://.*', '', line)) \\\n",
    "    .mapValues(lambda line: re.split('\\W+', line)) \\\n",
    "    .flatMap(lambda line: [(line[0], word) for word in line[1]]) \\\n",
    "    .filter(lambda line : len(line[1]) > 3) \\\n",
    "    .map(lambda line: ((line[0], line[1].lower()), 1)) \\\n",
    "    .reduceByKey(lambda word1, word2: word1 + word2)\n",
    "\n",
    "news7 = spark_session.sparkContext.textFile(\"./data/health_twitter/latimeshealth.txt\") \\\n",
    "    .flatMap(lambda line: line.split('\\t')) \\\n",
    "    .map(lambda line: line.split('|')) \\\n",
    "    .map(lambda line: (line[1][26:], line[2])) \\\n",
    "    .mapValues(lambda line: re.sub('http://.*', '', line)) \\\n",
    "    .mapValues(lambda line: re.split('\\W+', line)) \\\n",
    "    .flatMap(lambda line: [(line[0], word) for word in line[1]]) \\\n",
    "    .filter(lambda line : len(line[1]) > 3) \\\n",
    "    .map(lambda line: ((line[0], line[1].lower()), 1)) \\\n",
    "    .reduceByKey(lambda word1, word2: word1 + word2)\n",
    "\n",
    "news1.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3781e49-cccf-4e3e-9242-30b45889ec0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53967\n"
     ]
    }
   ],
   "source": [
    "unioned = news1.union(news2) \\\n",
    "    .union(news3) \\\n",
    "    .union(news4) \\\n",
    "    .union(news5) \\\n",
    "    .union(news6) \\\n",
    "    .union(news7) \\\n",
    "    .persist()\n",
    "\n",
    "print(unioned.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f435bed-4b59-4ff7-8776-de3452794d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2015', 'breast', 6),\n",
       " ('2015', 'cancer', 186),\n",
       " ('2015', 'risk', 77),\n",
       " ('2015', 'test', 35),\n",
       " ('2015', 'devised', 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unioned_clear = unioned.reduceByKey(lambda word1, word2: word1 + word2) \\\n",
    "    .map(lambda line: ((line[0][0], line[0][1], int(line[1]))))\n",
    "\n",
    "unioned_clear.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdad4271-00c5-4be7-bbd8-8a0d6fb18c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+\n",
      "|year|      word|count|\n",
      "+----+----------+-----+\n",
      "|2015|    breast|    6|\n",
      "|2015|    cancer|  186|\n",
      "|2015|      risk|   77|\n",
      "|2015|      test|   35|\n",
      "|2015|   devised|    1|\n",
      "|2015|  workload|    5|\n",
      "|2015|   harming|    2|\n",
      "|2015|      care|  390|\n",
      "|2015|      poll|    7|\n",
      "|2015|     short|    4|\n",
      "|2015|    people|  158|\n",
      "|2015|     heart|   40|\n",
      "|2015|   greater|    8|\n",
      "|2015|  approach|   26|\n",
      "|2015|   against|   16|\n",
      "|2015| promising|   26|\n",
      "|2015| coalition|    1|\n",
      "|2015|undermined|    1|\n",
      "|2015|   doctors|   93|\n",
      "|2015|    review|   18|\n",
      "+----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "news_schema = StructType([StructField(\"year\", StringType(), True), \\\n",
    "                            StructField(\"word\", StringType(), True), \\\n",
    "                             StructField(\"count\", IntegerType(), True)])\n",
    "\n",
    "news_df = spark_session.createDataFrame(unioned_clear, news_schema)\n",
    "news_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47032dff-86c4-4342-8bf5-1d6c54266997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "조회할 년도를 선택하세요(2013-2015) :  2013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+\n",
      "|year|      word|count|\n",
      "+----+----------+-----+\n",
      "|2013|      your| 1240|\n",
      "|2013|healthtalk|  815|\n",
      "|2013|      with|  565|\n",
      "|2013|    health|  523|\n",
      "|2013|      more|  490|\n",
      "|2013|      that|  479|\n",
      "|2013|      this|  417|\n",
      "|2013|      have|  407|\n",
      "|2013|      what|  405|\n",
      "|2013|      from|  389|\n",
      "|2013|     today|  339|\n",
      "|2013|     study|  311|\n",
      "|2013|     about|  307|\n",
      "|2013|      here|  299|\n",
      "|2013|    cancer|  294|\n",
      "|2013|     foods|  277|\n",
      "|2013|      says|  270|\n",
      "|2013|      help|  267|\n",
      "|2013|     heart|  254|\n",
      "|2013|    weight|  254|\n",
      "+----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_year = input(\"조회할 년도를 선택하세요(2013-2015) : \")\n",
    "news_df.filter(news_df['year'] == input_year).orderBy('count', ascending=False).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3f492-a397-4a95-8fb1-7ec3ed02a24e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
